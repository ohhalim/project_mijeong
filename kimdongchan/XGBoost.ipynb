{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1c8e4ce-ea6c-4b2a-9ecd-39ee71dddd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost 정확도: 0.7933\n",
      "XGBoost 평균 제곱 오차: 0.2067\n"
     ]
    }
   ],
   "source": [
    "# 1. 라이브러리 불러오기\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "\n",
    "# 2. 데이터 로드 및 확인\n",
    "titanic = sns.load_dataset('titanic')\n",
    "\n",
    "# 2-3. describe 함수 설명\n",
    "# count: 데이터의 개수, nan이 아닌 값의 개수를 보여줍니다. \n",
    "# std: 데이터의 표준 편차로, 데이터가 평균에서 얼마나 퍼져있는지를 나타냅니다.\n",
    "# min: 데이터의 최솟값\n",
    "# 25%: 1사분위수, 데이터의 25%가 이 값 이하임을 나타냅니다.\n",
    "# 50%: 중앙값, 데이터의 중간값을 나타냅니다.\n",
    "# 75%: 3사분위수, 데이터의 75%가 이 값 이하임을 나타냅니다.\n",
    "# max: 데이터의 최댓값\n",
    "\n",
    "# 3-1. 결측치 처리\n",
    "titanic['age'] = titanic['age'].fillna(titanic['age'].median())  # inplace=True 대신 직접 할당\n",
    "titanic['embarked'] = titanic['embarked'].fillna(titanic['embarked'].mode()[0])  # inplace=True 대신 직접 할당\n",
    "\n",
    "# 3-2. 수치형으로 인코딩\n",
    "titanic['sex'] = titanic['sex'].map({'male': 0, 'female': 1})\n",
    "titanic['alive'] = titanic['alive'].map({'yes': 1, 'no': 0})\n",
    "titanic['embarked'] = titanic['embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
    "\n",
    "# 3-3. 새로운 feature 생성\n",
    "titanic['family_size'] = titanic['sibsp'] + titanic['parch'] + 1\n",
    "\n",
    "# 4-1. 모델 학습 준비\n",
    "titanic = titanic[['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked', 'family_size']]\n",
    "X = titanic.drop('survived', axis=1)  # feature\n",
    "y = titanic['survived']  # target\n",
    "\n",
    "# 데이터 스케일링\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 5. 모델 학습 및 평가\n",
    "# 5-1. 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 5-2. XGBoost 모델 학습\n",
    "xgb = XGBClassifier(random_state=42)\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# 5-3. 예측 및 평가\n",
    "y_pred = xgb.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f'XGBoost 정확도: {accuracy:.4f}')\n",
    "print(f'XGBoost 평균 제곱 오차: {mse:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a333a4b3-4c9f-4f9e-8831-c9048e325b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최적의 파라미터: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}\n",
      "XGBoost 정확도: 0.8268\n",
      "XGBoost 평균 제곱 오차: 0.1732\n"
     ]
    }
   ],
   "source": [
    "# 5-2. XGBoost 하이퍼파라미터 튜닝\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(XGBClassifier(random_state=42), param_grid, scoring='accuracy', cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 최적의 파라미터로 모델 재학습\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# 5-3. 평가\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f'최적의 파라미터: {grid_search.best_params_}')\n",
    "print(f'XGBoost 정확도: {accuracy:.4f}')\n",
    "print(f'XGBoost 평균 제곱 오차: {mse:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88fdf402-4538-4c8b-a49c-8e9d7554bdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최적의 파라미터: {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 300, 'subsample': 1.0}\n",
      "앙상블 모델 정확도: 0.8268\n",
      "앙상블 모델 평균 제곱 오차: 0.1732\n"
     ]
    }
   ],
   "source": [
    "# 5-2. XGBoost 하이퍼파라미터 튜닝\n",
    "param_grid = {\n",
    "    'n_estimators': [200, 300],\n",
    "    'max_depth': [4, 5, 6],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]  # 새로운 하이퍼파라미터 추가\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(XGBClassifier(random_state=42), param_grid, scoring='accuracy', cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 최적의 파라미터로 모델 재학습\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# 5-3. 앙상블 모델 구축\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "voting_clf = VotingClassifier(estimators=[('xgb', best_model), ('rf', rf)], voting='soft')\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# 5-4. 예측 및 평가\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f'최적의 파라미터: {grid_search.best_params_}')\n",
    "print(f'앙상블 모델 정확도: {accuracy:.4f}')\n",
    "print(f'앙상블 모델 평균 제곱 오차: {mse:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb213ac5-18f2-4ce7-8c2b-99d06e4dbebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "survived         0\n",
      "pclass           0\n",
      "sex              0\n",
      "age            177\n",
      "sibsp            0\n",
      "parch            0\n",
      "fare             0\n",
      "embarked         2\n",
      "class            0\n",
      "who              0\n",
      "adult_male       0\n",
      "deck           688\n",
      "embark_town      2\n",
      "alive            0\n",
      "alone            0\n",
      "dtype: int64\n",
      "최적의 파라미터: {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 300, 'subsample': 0.7}\n",
      "앙상블 모델 정확도: 0.8324\n",
      "앙상블 모델 평균 제곱 오차: 0.1676\n"
     ]
    }
   ],
   "source": [
    "# 1. 라이브러리 불러오기\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# 데이터 로드 및 확인\n",
    "titanic = sns.load_dataset('titanic')\n",
    "\n",
    "# 결측치 확인\n",
    "print(titanic.isnull().sum())\n",
    "\n",
    "# 결측치 처리\n",
    "titanic['age'] = titanic['age'].fillna(titanic['age'].median())\n",
    "titanic['fare'] = titanic['fare'].fillna(titanic['fare'].median())\n",
    "titanic['embarked'] = titanic['embarked'].fillna(titanic['embarked'].mode()[0])\n",
    "\n",
    "# 수치형으로 인코딩\n",
    "titanic['sex'] = titanic['sex'].map({'male': 0, 'female': 1})\n",
    "titanic['alive'] = titanic['alive'].map({'yes': 1, 'no': 0})\n",
    "titanic['embarked'] = titanic['embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
    "\n",
    "# 새로운 feature 생성\n",
    "titanic['family_size'] = titanic['sibsp'] + titanic['parch'] + 1\n",
    "\n",
    "# 모델 학습 준비\n",
    "X = titanic[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked', 'family_size']]\n",
    "y = titanic['survived']\n",
    "\n",
    "# 데이터 스케일링\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 5. 모델 학습 및 평가\n",
    "# 5-1. 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 5-2. XGBoost 하이퍼파라미터 튜닝\n",
    "param_grid = {\n",
    "    'n_estimators': [300, 400],\n",
    "    'max_depth': [4, 5, 6, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.7, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.7, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(XGBClassifier(random_state=42), param_grid, scoring='accuracy', cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 최적의 파라미터로 모델 재학습\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# 5-3. 앙상블 모델 구축\n",
    "rf = RandomForestClassifier(n_estimators=150, random_state=42)\n",
    "voting_clf = VotingClassifier(estimators=[('xgb', best_model), ('rf', rf)], voting='soft')\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# 5-4. 예측 및 평가\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f'최적의 파라미터: {grid_search.best_params_}')\n",
    "print(f'앙상블 모델 정확도: {accuracy:.4f}')\n",
    "print(f'앙상블 모델 평균 제곱 오차: {mse:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a4e4e0-9be2-4b6f-87bc-0454dbe533f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
